{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import wget \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-02-21 17:31:47.908619: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-02-21 17:31:47.950246: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-02-21 17:31:47.950541: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-02-21 17:31:47.951472: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-02-21 17:31:47.953481: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-02-21 17:31:47.954049: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-02-21 17:31:47.954212: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-02-21 17:31:48.621116: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-02-21 17:31:48.621310: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-02-21 17:31:48.621439: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-02-21 17:31:48.621594: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 4115 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3060 Laptop GPU, pci bus id: 0000:01:00.0, compute capability: 8.6\n",
      "2023-02-21 17:31:49.899040: I tensorflow/stream_executor/cuda/cuda_blas.cc:1774] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(50,)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.layers import Dense\n",
    "\n",
    "X = np.zeros(shape=(1, 200))\n",
    "layer = Dense(units=50)\n",
    "z = layer(X)\n",
    "\n",
    "np.array(layer.bias).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(TensorShape([200, 50]), TensorShape([50]))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layer.weights[0].shape, layer.weights[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((200, 50), (50,))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = np.zeros(shape=(200))\n",
    "W = np.ones(shape=(200, 50))\n",
    "b = np.zeros(shape=(50))\n",
    "\n",
    "T = W + b\n",
    "\n",
    "Z = np.dot(X, W) + b\n",
    "\n",
    "T.shape, Z.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Layer Perceptron\n",
    "- init all Layer shape with list\n",
    "- Create forward Prop\n",
    "- Create Backward Prop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from keras.initializers import GlorotUniform\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "\n",
    "def initialize_weight(input_shape : int, output_shape: int):\n",
    "        \"\"\"Initialize Layer Weight with Xavier Initialization\n",
    "\n",
    "        Args:\n",
    "            input_shape (int): dimension of input\n",
    "            output_shape (int): dimension of output\n",
    "\n",
    "        Returns:\n",
    "            W : Numpy Array Weight of Layer\n",
    "        \"\"\"\n",
    "        limit_value = np.sqrt(6 / (input_shape + output_shape))\n",
    "\n",
    "        min_value = -limit_value\n",
    "        max_value = limit_value\n",
    "        \n",
    "        W = np.random.uniform(low =min_value,\n",
    "                              high=max_value,\n",
    "                              size=(input_shape, output_shape))\n",
    "        W = W.astype(np.float32)\n",
    "\n",
    "        return (W)\n",
    "    \n",
    "def initialize_bias(weight_output_shape : int):\n",
    "    \"\"\"Initialize the Bias Vector with Zeros\n",
    "\n",
    "    Args:\n",
    "        weight_output_shape  (int): output dimension of Weight Matrix\n",
    "\n",
    "    Returns:\n",
    "        b : Numpy Array of Zeros\n",
    "    \"\"\"\n",
    "    b = np.zeros(shape=(weight_output_shape))\n",
    "\n",
    "    return (b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.nn import softmax\n",
    "\n",
    "class Sigmoid:\n",
    "    \n",
    "    def __init__(self):\n",
    "        pass\n",
    "    def __call__(self, x):\n",
    "        return (1 / (1 + tf.exp(- x)))\n",
    "\n",
    "class ReLU:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    def __call__(self, x):\n",
    "        return (tf.maximum(0, x))\n",
    "\n",
    "class SoftMax:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    def __call__(self, x):\n",
    "        return (softmax(x))\n",
    "\n",
    "class Linear:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    def __call__(self, x):\n",
    "        return (x)\n",
    "    \n",
    "def choose_activation(name : str):\n",
    "\n",
    "    if (name == \"sigmoid\"):\n",
    "        Activation = Sigmoid()\n",
    "\n",
    "    elif (name == \"relu\"):\n",
    "        Activation = ReLU()\n",
    "\n",
    "    elif (name == \"softmax\"):\n",
    "        Activation = SoftMax()\n",
    "\n",
    "    else:\n",
    "        Activation = Linear()\n",
    "    \n",
    "    return (Activation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Layer:\n",
    "    \n",
    "        def __init__(self, input_shape : int, units :int, activation : str):\n",
    "            \n",
    "            self.input_shape = input_shape\n",
    "            self.output_shape = units\n",
    "            self.W = initialize_weight(input_shape, units)\n",
    "            self.b = initialize_bias(weight_output_shape=self.W.shape[1])\n",
    "            self.activation = choose_activation(activation)\n",
    "            self.A = None\n",
    "            self.Z = None\n",
    "        \n",
    "        def __call__(self, X):\n",
    "            self.Z = np.dot(X, self.W) + self.b\n",
    "            self.A = self.activation(self.Z)\n",
    "            return (self.A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Shape : 200 Output Shape : 50\n",
      "Input Shape : 50 Output Shape : 100\n",
      "Input Shape : 100 Output Shape : 150\n",
      "Input Shape : 150 Output Shape : 10\n"
     ]
    }
   ],
   "source": [
    "network_config = [\n",
    "    Layer(input_shape=200, units=50, activation=\"relu\"),\n",
    "    Layer(input_shape=50, units=100, activation=\"relu\"),\n",
    "    Layer(input_shape=100, units=150, activation=\"relu\"),\n",
    "    Layer(input_shape=150, units=10, activation=\"softmax\"),\n",
    "]\n",
    "\n",
    "for layer in network_config:\n",
    "    print(f\"Input Shape : {layer.input_shape} Output Shape : {layer.output_shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<__main__.Layer object at 0x7f11dc134a30>\n",
      "<__main__.Layer object at 0x7f11e3b74730>\n",
      "<__main__.Layer object at 0x7f11dc134ca0>\n",
      "<__main__.Layer object at 0x7f11e3b74190>\n"
     ]
    }
   ],
   "source": [
    "for layer in network_config:\n",
    "    print(layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tabulate import tabulate\n",
    "\n",
    "class MultiLayerPerceptron:\n",
    "\n",
    "        def __init__(self, input_shape: int, network_config : list):\n",
    "\n",
    "                self.input_shape = input_shape\n",
    "                self.network = network_config\n",
    "        \n",
    "        def summary(self):\n",
    "                print(\"Model\")\n",
    "                print(\"===============================================\")\n",
    "                \n",
    "                total_params = 0\n",
    "                table = list()\n",
    "                headers=['Name', 'Output Shape', 'Number Params']\n",
    "\n",
    "                input_layer = [\"Input Layer\", f\"(None, {self.input_shape})\", \"0\"]\n",
    "                table.append(input_layer)\n",
    "                \n",
    "                for index, layer in enumerate(self.network):\n",
    "                        \n",
    "                        if index == len(self.network) - 1:\n",
    "                                name = \"Output Layer\"\n",
    "                        else :\n",
    "                                name = f\"layer {index + 1}\"\n",
    "\n",
    "                        output_shape = f\"(None, {layer.output_shape})\"\n",
    "                        nbr_params = (layer.W.shape[0] * layer.W.shape[1])\n",
    "\n",
    "                        element = [name, output_shape, f\"{nbr_params:,}\"]\n",
    "                        \n",
    "                        total_params += nbr_params\n",
    "                        \n",
    "                        table.append(element)\n",
    "                        \n",
    "                print(tabulate(table, headers))\n",
    "                print(\"===============================================\")\n",
    "                print(f\"Total Params : {total_params:,}\")\n",
    "                \n",
    "        def forward(self, input_data):\n",
    "                \n",
    "                outputs = list()\n",
    "                for index, layer in enumerate(self.network):\n",
    "                        \n",
    "                        if index == 0:\n",
    "                                X = input_data\n",
    "                        else :\n",
    "                                last_layer = self.network[index - 1]\n",
    "                                last_activation = last_layer.A\n",
    "                                X = last_activation\n",
    "                        \n",
    "                        output = layer(X)\n",
    "                \n",
    "                return (output)\n",
    "                        \n",
    "        \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "Model: \"model\"\n",
    "_________________________________________________________________\n",
    " Layer (type)                Output Shape              Param #   \n",
    "=================================================================\n",
    " input_1 (InputLayer)        [(None, 30, 2)]           0         \n",
    "                                                                 \n",
    " flatten (Flatten)           (None, 60)                0         \n",
    "                                                                 \n",
    " dense (Dense)               (None, 128)               7808      \n",
    "                                                                 \n",
    " dense_1 (Dense)             (None, 7)                 903       \n",
    "                                                                 \n",
    " reshape (Reshape)           (None, 7, 1)              0         \n",
    "                                                                 \n",
    "=================================================================\n",
    "Total params: 8,711\n",
    "Trainable params: 8,711\n",
    "Non-trainable params: 0\n",
    "_________________________________________________________________\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.zeros(shape=(1, 200))\n",
    "model = MultiLayerPerceptron(input_shape=X.shape[1],\n",
    "                             network_config=network_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model\n",
      "===============================================\n",
      "Name          Output Shape    Number Params\n",
      "------------  --------------  ---------------\n",
      "Input Layer   (None, 200)     0\n",
      "layer 1       (None, 50)      10,000\n",
      "layer 2       (None, 100)     5,000\n",
      "layer 3       (None, 150)     15,000\n",
      "Output Layer  (None, 10)      1,500\n",
      "===============================================\n",
      "Total Params : 31,500\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10,)\n"
     ]
    }
   ],
   "source": [
    "activations = model.forward(X)\n",
    "\n",
    "for a in activations:\n",
    "        print(a.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf_gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "7af6488317c4eae45cfe2d92ddcd760ac10ac76eee454fa0eead8075769044a8"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
