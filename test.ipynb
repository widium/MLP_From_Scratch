{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.nn import softmax\n",
    "\n",
    "class Sigmoid:\n",
    "    \n",
    "    def __init__(self):\n",
    "        pass\n",
    "    def __call__(self, x):\n",
    "        return (1 / (1 + tf.exp(- x)))\n",
    "\n",
    "class ReLU:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    def __call__(self, x):\n",
    "        return (tf.maximum(0, x))\n",
    "\n",
    "class SoftMax:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    def __call__(self, x):\n",
    "        return (softmax(x).numpy())\n",
    "\n",
    "class Linear:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    def __call__(self, x):\n",
    "        return (x)\n",
    "    \n",
    "def choose_activation(name : str):\n",
    "\n",
    "    if (name == \"sigmoid\"):\n",
    "        Activation = Sigmoid()\n",
    "\n",
    "    elif (name == \"relu\"):\n",
    "        Activation = ReLU()\n",
    "\n",
    "    elif (name == \"softmax\"):\n",
    "        Activation = SoftMax()\n",
    "\n",
    "    else:\n",
    "        Activation = Linear()\n",
    "    \n",
    "    return (Activation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import wget \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-07 16:55:26.629568: I tensorflow/stream_executor/cuda/cuda_blas.cc:1774] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(50,)"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.layers import Dense\n",
    "\n",
    "X = np.zeros(shape=(1, 200))\n",
    "layer = Dense(units=50)\n",
    "z = layer(X)\n",
    "\n",
    "np.array(layer.bias).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(TensorShape([200, 50]), TensorShape([50]))"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layer.weights[0].shape, layer.weights[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((200, 50), (50,))"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = np.zeros(shape=(200))\n",
    "W = np.ones(shape=(200, 50))\n",
    "b = np.zeros(shape=(50))\n",
    "\n",
    "T = W + b\n",
    "\n",
    "Z = np.dot(X, W) + b\n",
    "\n",
    "T.shape, Z.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Layer Perceptron\n",
    "- init all Layer shape with list\n",
    "- Create forward Prop\n",
    "- Create Backward Prop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from keras.initializers import GlorotUniform\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "\n",
    "def initialize_weight(input_shape : int, output_shape: int):\n",
    "        \"\"\"Initialize Layer Weight with Xavier Initialization\n",
    "\n",
    "        Args:\n",
    "            input_shape (int): dimension of input\n",
    "            output_shape (int): dimension of output\n",
    "\n",
    "        Returns:\n",
    "            W : Numpy Array Weight of Layer\n",
    "        \"\"\"\n",
    "        limit_value = np.sqrt(6 / (input_shape + output_shape))\n",
    "\n",
    "        min_value = -limit_value\n",
    "        max_value = limit_value\n",
    "        \n",
    "        W = np.random.uniform(low =min_value,\n",
    "                              high=max_value,\n",
    "                              size=(input_shape, output_shape))\n",
    "        W = W.astype(np.float32)\n",
    "\n",
    "        return (W)\n",
    "    \n",
    "def initialize_bias(weight_output_shape : int):\n",
    "    \"\"\"Initialize the Bias Vector with Zeros\n",
    "\n",
    "    Args:\n",
    "        weight_output_shape  (int): output dimension of Weight Matrix\n",
    "\n",
    "    Returns:\n",
    "        b : Numpy Array of Zeros\n",
    "    \"\"\"\n",
    "    b = np.zeros(shape=(weight_output_shape))\n",
    "\n",
    "    return (b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Layer:\n",
    "    \n",
    "        def __init__(self, input_shape : int, units :int, activation : str):\n",
    "            \n",
    "            self.input_shape = input_shape\n",
    "            self.output_shape = units\n",
    "            self.W = initialize_weight(input_shape, units)\n",
    "            self.b = initialize_bias(weight_output_shape=self.W.shape[1])\n",
    "            self.activation = choose_activation(activation)\n",
    "            self.A = None\n",
    "            self.Z = None\n",
    "            \n",
    "            self.grads = dict()\n",
    "            self.grads[\"dA_next\"] = None\n",
    "            self.grads[\"dZ\"] = None\n",
    "            self.grads[\"dW\"] = None\n",
    "            self.grads[\"db\"] = None\n",
    "        \n",
    "        def __call__(self, X):\n",
    "            self.Z = np.dot(X, self.W) + self.b\n",
    "            self.A = self.activation(self.Z)\n",
    "            return (self.A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Shape : 200 Output Shape : 50\n",
      "Input Shape : 50 Output Shape : 100\n",
      "Input Shape : 100 Output Shape : 150\n",
      "Input Shape : 150 Output Shape : 10\n"
     ]
    }
   ],
   "source": [
    "network_config = [\n",
    "    Layer(input_shape=200, units=50, activation=\"relu\"),\n",
    "    Layer(input_shape=50, units=100, activation=\"relu\"),\n",
    "    Layer(input_shape=100, units=150, activation=\"relu\"),\n",
    "    Layer(input_shape=150, units=10, activation=\"softmax\"),\n",
    "]\n",
    "\n",
    "for layer in network_config:\n",
    "    print(f\"Input Shape : {layer.input_shape} Output Shape : {layer.output_shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<__main__.Layer object at 0x7f8e13fd68b0>\n",
      "<__main__.Layer object at 0x7f8e30673cd0>\n",
      "<__main__.Layer object at 0x7f8e30671ca0>\n",
      "<__main__.Layer object at 0x7f8e18047c40>\n"
     ]
    }
   ],
   "source": [
    "for layer in network_config:\n",
    "    print(layer)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cost Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CategoricalCrossEntropy:\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.epsilon = 1e-15\n",
    "\n",
    "    def __call__(self, y_true, y_pred):\n",
    "        \n",
    "        self.y_true = y_true\n",
    "        self.y_pred = y_pred\n",
    "        \n",
    "        y_pred_log = np.log(y_pred + self.epsilon)\n",
    "        class_error = -np.sum(y_true * y_pred_log, axis=1)\n",
    "        return (class_error)\n",
    "    \n",
    "    def backward(self):\n",
    "        \n",
    "        dY = self.y_pred + self.epsilon\n",
    "        dYTrue = - self.y_true / dY\n",
    "        DLoss = dYTrue / self.y_pred\n",
    "        \n",
    "        return (DLoss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MeanAbsoluteError:\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.epsilon = 1e-15\n",
    "    \n",
    "    def __call__(self, y_true, y_pred):\n",
    "        \n",
    "        self.y_true = y_true\n",
    "        self.y_pred = y_pred\n",
    "        \n",
    "        error = np.mean(np.abs(y_pred - y_true))\n",
    "        \n",
    "        return (error)\n",
    "    \n",
    "    def backward(self):\n",
    "        \n",
    "        batch_size = self.y_true.shape[0]\n",
    "        \n",
    "        diff = self.y_true - self.y_pred\n",
    "        \n",
    "        grad = np.zeros_like(diff)\n",
    "        grad[diff > 0] = -1\n",
    "        grad[diff < 0] = 1\n",
    "        \n",
    "        random_grad = np.random.uniform(low=-1, high=1, size=np.count_nonzero(diff == 0))\n",
    "        grad[diff == 0] = random_grad\n",
    "        \n",
    "        grad /= batch_size\n",
    "        \n",
    "        return (grad)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tabulate import tabulate\n",
    "\n",
    "class MultiLayerPerceptron:\n",
    "\n",
    "        def __init__(self, input_shape: int, network_config : list, loss_function):\n",
    "\n",
    "                self.input_shape = input_shape\n",
    "                self.network = network_config\n",
    "                self.loss_function = loss_function\n",
    "                self.lastActivation = None\n",
    "        \n",
    "        def summary(self):\n",
    "                print(\"Model\")\n",
    "                print(\"===============================================\")\n",
    "                \n",
    "                total_params = 0\n",
    "                table = list()\n",
    "                headers=['Name', 'Output Shape', 'Number Params']\n",
    "\n",
    "                input_layer = [\"Input Layer\", f\"(None, {self.input_shape})\", \"0\"]\n",
    "                table.append(input_layer)\n",
    "                \n",
    "                for index, layer in enumerate(self.network):\n",
    "                        \n",
    "                        if index == len(self.network) - 1:\n",
    "                                name = \"Output Layer\"\n",
    "                        else :\n",
    "                                name = f\"layer {index + 1}\"\n",
    "\n",
    "                        output_shape = f\"(None, {layer.output_shape})\"\n",
    "                        nbr_params = (layer.W.shape[0] * layer.W.shape[1])\n",
    "\n",
    "                        element = [name, output_shape, f\"{nbr_params:,}\"]\n",
    "                        \n",
    "                        total_params += nbr_params\n",
    "                        \n",
    "                        table.append(element)\n",
    "                        \n",
    "                print(tabulate(table, headers))\n",
    "                print(\"===============================================\")\n",
    "                print(f\"Total Params : {total_params:,}\")\n",
    "                \n",
    "        def forward(self, input_data):\n",
    "                \n",
    "                outputs = list()\n",
    "                for index, layer in enumerate(self.network):\n",
    "                        \n",
    "                        if index == 0:\n",
    "                                X = input_data\n",
    "                        else :\n",
    "                                last_layer = self.network[index - 1]\n",
    "                                last_activation = last_layer.A\n",
    "                                X = last_activation\n",
    "                        \n",
    "                        output = layer(X)\n",
    "                        \n",
    "                self.lastActivation = output\n",
    "                \n",
    "                return (self.lastActivation)\n",
    "        \n",
    "        def backward(self, y):\n",
    "                \n",
    "                ### Last Activation\n",
    "                # Initializing Backprop with Loss Backward\n",
    "                Dloss = self.loss_function.backward()\n",
    "                \n",
    "                \n",
    "                return (y)\n",
    "        \n",
    "        # def fit(self):\n",
    "                        \n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.random.uniform(size=(1, 200))\n",
    "\n",
    "model = MultiLayerPerceptron(input_shape=X.shape[1],\n",
    "                             network_config=network_config,\n",
    "                             loss_function=CategoricalCrossEntropy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([2.30258509]),\n",
       " array([[-100.,   -0.,   -0.,   -0.,   -0.,   -0.,   -0.,   -0.,   -0.,\n",
       "           -0.]]))"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss_function = CategoricalCrossEntropy()\n",
    "\n",
    "y_hat = model.forward(input_data=X)\n",
    "y_true = np.array([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
    "\n",
    "loss = loss_function(y_true=y_true, y_pred=y_hat)\n",
    "\n",
    "loss, loss_function.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model\n",
      "===============================================\n",
      "Name          Output Shape    Number Params\n",
      "------------  --------------  ---------------\n",
      "Input Layer   (None, 200)     0\n",
      "layer 1       (None, 50)      10,000\n",
      "layer 2       (None, 100)     5,000\n",
      "layer 3       (None, 150)     15,000\n",
      "Output Layer  (None, 10)      1,500\n",
      "===============================================\n",
      "Total Params : 31,500\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10,)\n"
     ]
    }
   ],
   "source": [
    "activations = model.forward(X)\n",
    "\n",
    "for a in activations:\n",
    "        print(a.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf_gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "7af6488317c4eae45cfe2d92ddcd760ac10ac76eee454fa0eead8075769044a8"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
